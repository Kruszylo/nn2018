{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout and Batch Normalization\n",
    "\n",
    "In this notebook we will implement Dropout and Batch Normalization. Batch Normalization both improves generalization and training speed. Dropout is considered to be a regularization technique.\n",
    "\n",
    "References:\n",
    "* \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\": https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "* \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\": https://arxiv.org/abs/1502.03167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src import fmnist_utils\n",
    "from src.fmnist_utils import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd.function import InplaceFunction\n",
    "from torch.autograd import Variable\n",
    "from itertools import repeat\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data(which=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboard exercises\n",
    "\n",
    "* (1.0) Write equations for training and inference phase of Dropout. Explain inference mode of Dropout (see https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf, p 1531)\n",
    "* (0.5) Write equations for training and inference phase of Batch Normalization\n",
    "\n",
    "(And any left-out exercise from the previous lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Note: We use MNIST to be able to quickly and in a stable way fit data, which makes it a good target for studying generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement and show better generalization of Dropout\n",
    "\n",
    "Answer the following questions using the provided starting code and model:\n",
    "\n",
    "a) What train and test accuracy did you achieve without dropout?\n",
    "\n",
    "b) What train and test accuracy did you achieve with dropout (best alpha)?\n",
    "\n",
    "Hint: use torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BernoulliDropout(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(BernoulliDropout, self).__init__()\n",
    "        self.alpha = torch.Tensor([alpha])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise e ~ B(alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            ??\n",
    "        else:\n",
    "            ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\"}\n",
    "json.dump(answers, open(\"8_ex1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Starting code\n",
    "\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "alpha = 0.00\n",
    "hidden_dims =  [50, 50, 50]\n",
    "model = torch.nn.Sequential()\n",
    "previous_dim = input_dim\n",
    "for id, D in enumerate(hidden_dims):\n",
    "    model.add_module(\"dropout_{}\".format(id), BernoulliDropout(alpha))\n",
    "    model.add_module(\"linear_{}\".format(id), torch.nn.Linear(previous_dim, D, bias=True))\n",
    "    model.add_module(\"nonlinearity_{}\".format(id), torch.nn.ReLU())\n",
    "    previous_dim = D\n",
    "model.add_module(\"final_layer\", torch.nn.Linear(D, output_dim, bias=True))\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "          x_test=x_test, y_test=y_test,\n",
    "          optim=optimizer, batch_size=128, n_epochs=100)\n",
    "\n",
    "plot(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Does it really reduce internal drift?\n",
    "\n",
    "Produce an experiment similar to Figure 1 in https://arxiv.org/pdf/1502.03167.pdf (you can use our data and neural network). Result is just a figure, save it to 8_3.png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151e7310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"8_3.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
