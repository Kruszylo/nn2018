{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout and Batch Normalization\n",
    "\n",
    "In this notebook we will implement Dropout and Batch Normalization. Batch Normalization both improves generalization and training speed. Dropout is considered to be a regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src import fmnist_utils\n",
    "from src.fmnist_utils import *\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction\n",
    "from torch.autograd import Variable\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(InplaceFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_noise(input):\n",
    "        return input.new().resize_as_(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, input, p=0.5, train=False, inplace=False):\n",
    "        # See Note [Export inplace]\n",
    "        r, _ = g.op(\"Dropout\", input, ratio_f=p, is_test_i=not train, outputs=2)\n",
    "        return r\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls, ctx, input, p=0.5, train=False, inplace=False):\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        ctx.p = p\n",
    "        ctx.train = train\n",
    "        ctx.inplace = inplace\n",
    "\n",
    "        if ctx.p == 0 or not ctx.train:\n",
    "            return input\n",
    "\n",
    "        if ctx.inplace:\n",
    "            ctx.mark_dirty(input)\n",
    "            output = input\n",
    "        else:\n",
    "            output = input.clone()\n",
    "\n",
    "        ctx.noise = cls._make_noise(input)\n",
    "        if ctx.p == 1:\n",
    "            ctx.noise.fill_(0)\n",
    "        else:\n",
    "            ctx.noise.bernoulli_(1 - ctx.p).div_(1 - ctx.p)\n",
    "        ctx.noise = ctx.noise.expand_as(input)\n",
    "        output.mul_(ctx.noise)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.p > 0 and ctx.train:\n",
    "            return grad_output * ctx.noise, None, None, None\n",
    "        else:\n",
    "            return grad_output, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims =  []\n",
    "model = torch.nn.Sequential()\n",
    "previous_dim = input_dim\n",
    "for id, D in enumerate(hidden_dims):\n",
    "    model.add_module(\"linear_{}\".format(id), torch.nn.Linear(previous_dim, D, bias=True))\n",
    "    model.add_module(\"nonlinearity_{}\".format(id), torch.nn.ReLU())\n",
    "    previous_dim = D\n",
    "model.add_module(\"final_layer\", torch.nn.Linear(D, output_dim, bias=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
