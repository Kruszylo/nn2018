{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In this notebook we will implement Conv2D layer.\n",
    "\n",
    "Goal of this lab is to:\n",
    "\n",
    "* Implement and understand basic aspects of Convolutions\n",
    "\n",
    "References:\n",
    "* Largely based on http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code to get started\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src import fmnist_utils\n",
    "from src.fmnist_utils import *\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/401_CNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=300 src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\">\n",
    "<img width=300 src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
    "\n",
    "See animation at http://cs231n.github.io/convolutional-networks/, section \"Convolution Demo\".\n",
    "\n",
    "Summary. To summarize, the Conv Layer:\n",
    "\n",
    "* Accepts a volume of size W1×H1×D1\n",
    "* Requires four hyperparameters:\n",
    "    - Number of filters K,\n",
    "    - their spatial extent F,\n",
    "    - the stride S,\n",
    "    - the amount of zero padding P.\n",
    "* Produces a volume of size W2×H2×D2 where:\n",
    "    - W2=(W1−F+2P)/S+1\n",
    "    - H2=(H1−F+2P)/S+1 (i.e. width and height are computed equally by symmetry)\n",
    "    - D2=K\n",
    "    \n",
    "With parameter sharing, it introduces F⋅F⋅D1 weights per filter, for a total of (F⋅F⋅D1)⋅K weights and K biases.\n",
    "In the output volume, the d-th depth slice (of size W2×H2) is the result of performing a valid convolution of the d-th filter over the input volume with a stride of S, and then offset by d-th bias.\n",
    "A common setting of the hyperparameters is F=3,S=1,P=1. However, there are common conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboard exercises\n",
    "\n",
    "(Plus anything from the previous labs)\n",
    "\n",
    "* (0.5) Explain equation for volume size above (i.e. explain expression for W2, H2 and D2)\n",
    "* (1.0) Compared to a dense layer  (with the same amount of neurons), should intialization magnitude of weights of convolution to be larger or smaller? Explain intuition behind the answer. Hint: consider equation for popular initialization in DNN, e.g. Glorot.\n",
    "* (0.5) How does output of the convolutional layer react to small (e.g. 2px) shift of image?\n",
    "* (1.0) Are convolutional filters invariant to rotation of the input? If not, can you devise a simple strategy to encourage invariance rotation? Explain why your strategy should work. Hint: think outside of changing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implement foward pass of convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Conv2d(1, 16, kernel_size=5, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pytorch_conv2d_foward(input, kernel, padding):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
