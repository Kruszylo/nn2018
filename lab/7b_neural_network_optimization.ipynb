{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Neural Networks\n",
    "\n",
    "To remind, necessary ingredients to train NN:\n",
    "    * model\n",
    "    * objective\n",
    "    * optimizer\n",
    "    \n",
    "Today we will try to understand basics of optimization of neural networks, giving context for the last two lectures. Goal is to:\n",
    "* Understand basics of generalization, and the difference between optimization and generalization (more on that in \"Understanding generalization\" lab)\n",
    "* Understand impact of hyperparameters in SGD on:\n",
    "\n",
    "  - generalization (lr, batch size)\n",
    "  - speed of optimization (lr, momentum, batch size) \n",
    "\n",
    "References:\n",
    "* Deep Learning book chapter on optimization: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code to get started\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src.fmnist_utils import build_mlp, train\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: optimization speed\n",
    "\n",
    "Assuming fixed number of *epochs*, it is usually better to use either smaller batch size, or larger learning rate. Theoretical reason for it is not completely clear, so let's focus in this exercise on an empirical investigation.\n",
    "\n",
    "Assume you are allowed to train the given network for 10 epochs. Answer the following questions:\n",
    "\n",
    "* a) What was the optimal $\\eta$ (assuming $S$=128 and $\\mu$=0.9) for the final training accuracy?\n",
    "* b) Did it also provide the best test accuracy? If yes, why (hint: consider if model is under or over-fitting)?\n",
    "* c) What is the optimal $S$ (assuming $\\eta$=0.1 and $\\mu$=0.9) for the final training accuracy?\n",
    "* d) Why is higher learning rate, or smaller batch size, optimizing faster? Give your best explanation (it can be hypothetical, there is no obvious theoretical answer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some starting code. Loop through different LR and BS and find optimal ones.\n",
    "model = build_mlp(784, 10, hidden_dim=512)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "          x_test=x_test, y_test=y_test,\n",
    "          optim=optimizer, batch_size=128, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}\n",
    "json.dump(answers, open(\"7b_ex1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: generalization\n",
    "\n",
    "Story with generalization is also unclear, but it is generally accepted that higher noise levels in SGD lead to better generalization. Think of noise in optimization (leading to low fidelity, as seen in lab 7a, for instance) as a close analog of typical regularizations (like dropout or batch normalization, that we will discuss next time).\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "a) Implement the stability measure described below\n",
    "\n",
    "b) Check a range of LR and BS and find the best generalizing combination of LR and BS. What test accuracy were you able to achieve?\n",
    "\n",
    "c) Answer the following question: Is stability correlated with using large LR or small BS. If yes, what is the intuitive reason for it? Feel free to give a hypothesis, there is no stricly correct or incorrect answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"c\": \"\"}\n",
    "json.dump(answers, open(\"7b_ex2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding generalization, take 2: stability\n",
    "\n",
    "In 7a lab we discussed bias/variance view. Here, we will take a stability based view. To estimate stability, \n",
    "we will record maximum change in prediction when adding gaussian noise to examples. This is a very rudimentary\n",
    "way to estimate geometric margin of the network, and we will talk more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, loss, optim, \n",
    "          x_train, y_train, x_test, y_test, batch_size=100, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains given model on the FashionMNIST dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history: dict\n",
    "        History containing 'acc' and 'test_acc' keys.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42) \n",
    "    n_examples, n_features = x_train.size()\n",
    "    history = {\"acc\": [], \"test_acc\": []}\n",
    "    for i in tqdm.tqdm(range(n_epochs), total=n_epochs):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += step(model, loss, optim, x_train[start:end], y_train[start:end])\n",
    "        \n",
    "        predY = predict(model, x_test)\n",
    "        test_acc = np.mean(predY == y_test.numpy())\n",
    "        history['test_acc'].append(test_acc)\n",
    " \n",
    "        # Usually it is computed from per batch averages, but I compute\n",
    "        # here using the whole train set to reduce level of noise in the learning curves\n",
    "        predY = predict(model, x_train)\n",
    "        train_acc = np.mean(predY == y_train.numpy())\n",
    "        history['acc'].append(train_acc)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some starting code. Loop through different LR and BS and find optimal ones.\n",
    "model = build_mlp(784, 10, hidden_dim=512)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "          x_test=x_test, y_test=y_test,\n",
    "          optim=optimizer, batch_size=128, n_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
