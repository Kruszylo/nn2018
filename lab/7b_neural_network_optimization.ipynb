{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Neural Networks\n",
    "\n",
    "To remind, necessary ingredients to train NN:\n",
    "    * model\n",
    "    * objective\n",
    "    * optimizer\n",
    "    \n",
    "Today we will try to understand basics of optimization of neural networks, giving context for the last two lectures. Goal is to:\n",
    "* Understand basics of generalization, and the difference between optimization and generalization (more on that in \"Understanding generalization\" lab)\n",
    "* Understand impact of hyperparameters in SGD on:\n",
    "\n",
    "  - generalization (lr, batch size)\n",
    "  - speed of optimization (lr, momentum, batch size) \n",
    "\n",
    "References:\n",
    "* Deep Learning book chapter on optimization: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code to get started\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src.fmnist_utils import build_mlp, train\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: optimization speed\n",
    "\n",
    "Assuming fixed number of *epochs*, it is usually better to use either smaller batch size, or larger learning rate. Theoretical reason for it is not completely clear, so let's focus in this exercise on empirical investigation.\n",
    "\n",
    "Assume you are allowed the given network for 10 epochs. Answer the following questions:\n",
    "\n",
    "* a) What was the optimal $\\eta$ (assuming $S$=128 and $\\mu$=0.9) for the final training accuracy?\n",
    "* b) Did it also provide the best test accuracy? If yes, why (hint: consider if model is under or over-fitting)?\n",
    "* c) What is the optimal $S$ (assuming $\\eta$=0.1 and $\\mu$=0.9) for the final training accuracy?\n",
    "* d) Why is higher learning rate, or smaller batch size, optimizing faster? Give your best explanation (it can be hypothetical, there is no obvious theoretical answer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some starting code. Loop through different LR and BS and find optimal ones.\n",
    "model = build_mlp(784, 10, hidden_dim=512)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "H = train(loss=loss, model=model, x_train=x_train, y_train=y_train,\n",
    "          x_test=x_test, y_test=y_test,\n",
    "          optim=optimizer, batch_size=128, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}\n",
    "json.dump(answers, open(\"7b_ex1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: generalization\n",
    "\n",
    "Story with generalization is also unclear, but it is generally accepted that higher noise levels in SGD lead to better generalization. Think of noise in optimization (leading to low fidelity, as seen in lab 7a, for instance) as a close analog of typical regularizations (like dropout or batch normalization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
