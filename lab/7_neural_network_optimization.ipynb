{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimizing Neural Networks\n",
    "\n",
    "To remind, necessary ingredients to train NN:\n",
    "    * model\n",
    "    * objective\n",
    "    * optimizer\n",
    "    \n",
    "Today we will try to understand basics of optimization of neural networks, giving context for the last two lectures. Goal is to:\n",
    "* Understand basics of generalization, and the difference between optimization and generalization (more on that in \"Understanding generalization\" lab)\n",
    "* Understand role of hyperparameters in SGD\n",
    "\n",
    "References:\n",
    "* Deep Learning book chapter on optimization: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training loop adapted from 4_computing_gradient.ipynb\n",
    "\n",
    "def build_mlp(input_dim, output_dim, hidden_dim=512):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_1\", torch.nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "    model.add_module(\"nonlinearity_1\", torch.nn.ReLU())\n",
    "    model.add_module(\"linear_2\", torch.nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "    return model\n",
    "\n",
    "def build_logreg(input_dim, output_dim, hidden_dim=512):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_2\", torch.nn.Linear(input_dim, output_dim, bias=False))\n",
    "    return model\n",
    "\n",
    "def step(model, loss, optimizer, x_val, y_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    y = Variable(y_val, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx, y)\n",
    "\n",
    "    # Backward\n",
    "    output.backward(retain_graph=True)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.data[0]\n",
    "\n",
    "def predict(model, x_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)\n",
    "\n",
    "def train(model, loss, optim):\n",
    "    \"\"\"\n",
    "    Trains given model on the FashionMNIST dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history: dict\n",
    "        History containing 'acc' and 'test_acc' keys.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    n_examples, n_features = x_train.size()\n",
    "    n_classes = 10\n",
    "    batch_size = 100\n",
    "    history = {\"acc\": [], \"test_acc\": []}\n",
    "    for i in tqdm.tqdm(range(100), total=100):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += step(model, loss, optim, x_train[start:end], y_train[start:end])\n",
    "        \n",
    "        predY = predict(model, x_test)\n",
    "        test_acc = np.mean(predY == y_test.numpy())\n",
    "        history['test_acc'].append(test_acc)\n",
    " \n",
    "        # Note: usually it is computed from per batch averages\n",
    "        predY = predict(model, x_train)\n",
    "        train_acc = np.mean(predY == y_train.numpy())\n",
    "        history['acc'].append(train_acc)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization 101: learning curves and bias/variance decomposition\n",
    "\n",
    "To talk about optimization, it is important to understand the difference between optimization and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is sort-of easy, in the sense it is much more important to generalize than to optimize (but the two co-incide in SGD, optimization gives usually good generalization):\n",
    "\n",
    "<img width=300 src=\"fig/7/overfittingspiral.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.83it/s]\n"
     ]
    }
   ],
   "source": [
    "model = build_mlp(784, 10)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "H = train(loss=loss, model=model, optim=optimizer)\n",
    "plot(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance\n",
    "\n",
    "One way to understand generalization in machine learnning is through the bias-variance tradeoff. Other ways will be discussed next time.\n",
    "\n",
    "<img width=200 src=\"fig/7/bias-and-variance.jpg\">\n",
    "\n",
    "<img width=600 src=\"fig/7/bias-and-variance2.png\">\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$E[(y - \\hat{f(x)})^2] = Bias[\\hat{f(x)}]^2 + Var[\\hat{f(x)}] + \\sigma^2$$, where\n",
    "\n",
    "$$Bias[f(x)] = E[\\hat{f(x)} - f(x)], Var[\\hat{f(x)}] = E[(\\hat{f(x)} - E[\\hat{f(x)}])^2]$$\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Train 10 different logistic regressions and MLPs (hidden_dim=100) using same optimization hyperparameters. Answer the following questions:\n",
    "    \n",
    "a) Which achieves the lowest test error?\n",
    "\n",
    "b) What is the *mean* test accuracy of MLP?\n",
    "\n",
    "c) Which has the lower variance, and how do you know?\n",
    "\n",
    "Additionally: plot train accuracy curve for all runs\n",
    "\n",
    "Please, save answers to json (as in the next cell). Do not change name of the json file.\n",
    "\n",
    "Hint:\n",
    "    * Use torch.manual_seed(seed) to set seed for each repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {\"a\": \"\", \"b\": \"\", \"c\": \"\"}\n",
    "json.dump(answers, open(\"7_ex1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 107.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 121.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 113.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 102.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.14it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 38.49it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 39.32it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 39.31it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 39.53it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 38.54it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {\"logreg\": [], \"mlp\": []}\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    model = build_logreg(784, 10, hidden_dim=hdim)\n",
    "    loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    H = train(loss=loss, model=model, optim=optimizer)\n",
    "    results[\"logreg\"].append(H)\n",
    "    \n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    model = build_mlp(784, 10, hidden_dim=100)\n",
    "    loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    H = train(loss=loss, model=model, optim=optimizer)\n",
    "    results[\"mlp\"].append(H)\n",
    "    \n",
    "np.std([max(H['test_acc']) for H in results[\"logreg\"]])\n",
    "np.mean([max(H['test_acc']) for H in results[\"logreg\"]])\n",
    "\n",
    "for H in results[\"mlp\"]:\n",
    "    plt.plot(H['acc'])\n",
    "    \n",
    "for H in results[\"logreg\"]:\n",
    "    plt.plot(H['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding curvature, SGD on Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Maximum LR and curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD(M) hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: impact of hyperparameters on learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune base LR and BS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
