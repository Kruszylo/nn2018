{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - part A\n",
    "\n",
    "In this notebook we will implement Conv2D layer.\n",
    "\n",
    "Goal of this lab is to:\n",
    "\n",
    "* Implement and understand basic aspects of Convolutions\n",
    "\n",
    "References:\n",
    "* Largely based on http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code to get started\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from src import fmnist_utils\n",
    "from src.fmnist_utils import *\n",
    "\n",
    "def plot(H):\n",
    "    plt.title(max(H['test_acc']))\n",
    "    plt.plot(H['acc'], label=\"acc\")\n",
    "    plt.plot(H['test_acc'], label=\"test_acc\")\n",
    "    plt.legend()\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fmnist_utils.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/401_CNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=300 src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\">\n",
    "<img width=300 src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
    "\n",
    "See animation at http://cs231n.github.io/convolutional-networks/, section \"Convolution Demo\".\n",
    "\n",
    "Summary. To summarize, the Conv Layer:\n",
    "\n",
    "* Accepts a volume of size W1×H1×D1\n",
    "* Requires four hyperparameters:\n",
    "    - Number of filters K,\n",
    "    - their spatial extent F,\n",
    "    - the stride S,\n",
    "    - the amount of zero padding P.\n",
    "* Produces a volume of size W2×H2×D2 where:\n",
    "    - W2=(W1−F+2P)/S+1\n",
    "    - H2=(H1−F+2P)/S+1 (i.e. width and height are computed equally by symmetry)\n",
    "    - D2=K\n",
    "    \n",
    "With parameter sharing, it introduces F⋅F⋅D1 weights per filter, for a total of (F⋅F⋅D1)⋅K weights and K biases.\n",
    "In the output volume, the d-th depth slice (of size W2×H2) is the result of performing a valid convolution of the d-th filter over the input volume with a stride of S, and then offset by d-th bias.\n",
    "A common setting of the hyperparameters is F=3,S=1,P=1. However, there are common conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboard exercises\n",
    "\n",
    "(Plus anything from the previous labs)\n",
    "\n",
    "* (0.5) Explain equation for volume size above (i.e. explain expression for W2, H2 and D2)\n",
    "* (1.0) Compared to a dense layer  (with the same amount of neurons), should intialization magnitude of weights of convolution to be larger or smaller? Explain intuition behind the answer. Hint: consider equation for popular initialization in DNN, e.g. Glorot.\n",
    "* (0.5) How does output of the convolutional layer react to small (e.g. 2px) shift of image?\n",
    "* (1.0) Are convolutional filters invariant to rotation of the input? If not, can you devise a simple strategy to encourage invariance rotation? Explain why your strategy should work. Hint: think outside of changing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implement foward pass of convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv2d_forward(input, kernel, padding, stride):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    input: torch.FloatTensor, shape (n_examples, n_channels, 28, 28)\n",
    "    kernel: torch.FloatTensor, shape (n_filters, 1, kernel_size, kernel_size)\n",
    "    padding: int\n",
    "        Padding to add\n",
    "    \"\"\"\n",
    "    # Dummy implementation sampling output with a correct shape\n",
    "    N = input.shape[0]\n",
    "    D = kernel.shape[0]\n",
    "    W, H = input.shape[2], input.shape[3]\n",
    "    F = kernel.shape[-1]\n",
    "    S = stride\n",
    "    P = padding\n",
    "    out = np.random.uniform(size=(N, D, (W-F+2*P)/S+1, (H-F+2*P)/S+1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pytorch_conv2d_foward(input, kernel, padding, stride):\n",
    "    # Ugly code to forward input through PyTorch convolution\n",
    "    assert kernel.shape[-2] == kernel.shape[-1]\n",
    "    kernel_size = kernel.shape[-1]\n",
    "    n_filters = kernel.shape[0]\n",
    "    n_channels = kernel.shape[1]\n",
    "    m = nn.Conv2d(n_channels, n_filters, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "    m.weight.data.copy_(torch.FloatTensor(kernel))\n",
    "    output = m.forward(Variable(input))\n",
    "    output = output.data.numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_conv2d(ex, w, P, S):\n",
    "    out_student = conv2d_forward(ex, w, P, S)\n",
    "    out = pytorch_conv2d_foward(ex, w, P, S)\n",
    "    result = np.allclose(out, out_student, atol=1e-2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex = x_train[0:40].view(40, 4, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14L, 5L)\n"
     ]
    }
   ],
   "source": [
    "w = torch.FloatTensor(np.random.uniform(size=(16, 4, 5, 5)))\n",
    "results['1'] = test_conv2d(ex, w, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(results, open(\"9a_conv.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
